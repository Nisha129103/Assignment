{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVQpBraoL7nQ9hn3S8jSdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nisha129103/Assignment/blob/main/Statistics_advance_1_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the properties of the F-distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "WFkRfN6HwAFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> The F-distribution, named after Sir Ronald Fisher, is a continuous probability distribution used in statistical hypothesis testing and regression analysis.\n",
        "\n",
        "Properties of the F-distribution:\n",
        "\n",
        "Right-skewed: The F-distribution is skewed to the right, meaning it has a longer tail on the right side.\n",
        "\n",
        "Non-symmetric: The distribution is not symmetric around its mean.\n",
        "\n",
        "Positive values only: The F-distribution only takes positive values.\n",
        "\n",
        "Two parameters: The F-distribution has two parameters: degrees of freedom (df1 and df2).\n",
        "\n",
        "Dependent on degrees of freedom: The shape of the distribution changes with df1 and df2.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Mean: The mean of the F-distribution is df2 / (df2 - 2) for df2 > 2.\n",
        "\n",
        "Variance: The variance is 2 * (df2^2 * (df1 + df2 - 2)) / (df1 * (df2 - 2)^2 * (df2 - 4)).\n",
        "\n",
        "Mode: The mode is typically around 1.\n",
        "\n",
        "Types of F-distributions:\n",
        "\n",
        "F(1, df2): Used for simple linear regression.\n",
        "F(df1, df2): Used for multiple linear regression and analysis of variance (ANOVA).\n",
        "Common Applications:\n",
        "\n",
        "Hypothesis testing: F-tests for regression coefficients, ANOVA, and analysis of covariance (ANCOVA).\n",
        "Regression analysis: Testing significance of regression coefficients.\n",
        "Analysis of variance: Comparing means across multiple groups.\n",
        "Relationship to Other Distributions:\n",
        "\n",
        "Chi-squared distribution: The F-distribution is related to the chi-squared distribution.\n",
        "t-distribution: The square of a t-statistic follows an F-distribution.\n"
      ],
      "metadata": {
        "id": "EIoz6ZzYwokb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "dkzfsBw4w2VY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> The F-distribution is used in various statistical tests, primarily in:\n",
        "\n",
        "Analysis of Variance (ANOVA):\n",
        "Tests equality of means across multiple groups.\n",
        "F-statistic compares variance between groups (SSB) to variance within groups (SSW).\n",
        "F-distribution is appropriate due to:\n",
        "Independence of SSB and SSW.\n",
        "Chi-squared distribution of SSB and SSW under normality.\n",
        "Regression Analysis:\n",
        "Tests significance of regression coefficients.\n",
        "F-statistic compares explained variance (RSS) to residual variance (RSE).\n",
        "F-distribution is appropriate due to:\n",
        "Linearity and independence of residuals.\n",
        "Normality of residuals.\n",
        "Analysis of Covariance (ANCOVA):\n",
        "Tests equality of means across multiple groups while controlling for covariates.\n",
        "F-statistic compares variance between groups to variance within groups.\n",
        "F-distribution is appropriate due to:\n",
        "Independence of SSB and SSW.\n",
        "Normality of residuals.\n",
        "Multiple Comparisons:\n",
        "Tests differences between multiple group means.\n",
        "F-statistic controls Type I error rate.\n",
        "F-distribution is appropriate due to:\n",
        "Independence of comparisons.\n",
        "Normality of data.\n",
        "Time Series Analysis:\n",
        "Tests significance of autoregressive (AR) or moving average (MA) coefficients.\n",
        "F-statistic compares explained variance to residual variance.\n",
        "F-distribution is appropriate due to:\n",
        "Linearity and independence of residuals.\n",
        "Normality of residuals.\n",
        "Why F-distribution is appropriate:\n",
        "\n",
        "Independence: F-distribution assumes independence of numerator and denominator.\n",
        "Normality: F-distribution assumes normality of residuals or data.\n",
        "Chi-squared distribution: F-distribution is related to chi-squared distribution.\n",
        "Ratio of variances: F-statistic compares variances, making F-distribution suitable.\n",
        "Assumptions:\n",
        "\n",
        "Normality of residuals or data.\n",
        "Independence of observations.\n",
        "Homoscedasticity (constant variance).\n",
        "Linearity (in regression analysis)."
      ],
      "metadata": {
        "id": "E4y0s8l5xLyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
      ],
      "metadata": {
        "id": "sxC_pj3XxRH8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> Conducting an F-test to compare variances requires:\n",
        "\n",
        "Key Assumptions:\n",
        "\n",
        "Normality: Data from both populations should follow a normal distribution.\n",
        "Independence: Observations within and between groups should be independent.\n",
        "Homoscedasticity: Variances within each population should be constant.\n",
        "Random Sampling: Samples should be randomly selected from each population.\n",
        "Equal Subpopulation Variances (optional): If testing for equal variances, assume σ1 = σ2.\n",
        "Additional Considerations:\n",
        "\n",
        "No significant outliers: Outliers can inflate variance estimates.\n",
        "No significant skewness: Skewed data can affect variance estimates.\n",
        "No significant correlation: Correlated data can affect independence.\n",
        "F-Test Specific Assumptions:\n",
        "\n",
        "Ratio of Variances: F-test assumes the ratio of variances (σ1² / σ2²) follows an F-distribution.\n",
        "Degrees of Freedom: F-test requires calculation of degrees of freedom (df1, df2) for each population.\n",
        "Consequences of Violating Assumptions:\n",
        "\n",
        "Inaccurate p-values: Incorrect conclusions about variance differences.\n",
        "Reduced test power: Failure to detect significant differences.\n",
        "Increased Type I error: Incorrect rejection of null hypothesis.\n",
        "Alternatives if Assumptions are Violated:\n",
        "\n",
        "Non-parametric tests (e.g., Levene's test, Brown-Forsythe test)\n",
        "Transformations (e.g., logarithmic, square root)\n",
        "Robust statistical methods (e.g., trimmed means)\n",
        "Q4. What is the purpose of ANOVA, and how does it differ from a t-test?\n",
        "\n",
        "ANS--> Purpose of ANOVA (Analysis of Variance):\n",
        "\n",
        "ANOVA is a statistical technique used to:\n",
        "\n",
        "Compare means of three or more groups.\n",
        "Determine if there are significant differences between group means.\n",
        "Analyze the relationship between a continuous outcome variable and one or more categorical predictor variables.\n",
        "Key Features of ANOVA:\n",
        "\n",
        "Compares multiple groups (3+).\n",
        "Assesses variance between and within groups.\n",
        "F-statistic measures the ratio of between-group variance to within-group variance.\n",
        "Comparison to t-test:\n",
        "\n",
        "t-test:\n",
        "\n",
        "Compares means of two groups.\n",
        "Assesses difference between two group means.\n",
        "t-statistic measures the difference between group means relative to standard error.\n",
        "Key differences:\n",
        "\n",
        "Number of groups: ANOVA (3+ groups) vs. t-test (2 groups).\n",
        "Variance analysis: ANOVA examines both between-group and within-group variance, while t-test focuses on difference between two means.\n",
        "Statistical power: ANOVA is more powerful for detecting differences among multiple groups.\n",
        "Types of ANOVA:\n",
        "\n",
        "One-way ANOVA: Compares means of three or more groups with one independent variable.\n",
        "Two-way ANOVA: Examines interactions between two independent variables.\n",
        "Repeated Measures ANOVA: Analyzes data from repeated measurements.\n",
        "Assumptions:\n",
        "\n",
        "Normality of residuals.\n",
        "Homoscedasticity (equal variances).\n",
        "Independence of observations.\n"
      ],
      "metadata": {
        "id": "3kLJv2SPxVAo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
      ],
      "metadata": {
        "id": "9WAuKQz6xaIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> Use one-way ANOVA when:\n",
        "\n",
        "Comparing means of three or more groups.\n",
        "Investigating the effect of a single categorical independent variable (factor) on a continuous outcome variable.\n",
        "Wanting to determine if there are significant differences between group means.\n",
        "Why prefer One-way ANOVA over Multiple t-tests:\n",
        "\n",
        "Control of Type I error rate: ANOVA protects against inflated Type I error rates (false positives) that occur when conducting multiple t-tests.\n",
        "\n",
        "Increased statistical power: ANOVA is more powerful for detecting differences among multiple groups.\n",
        "\n",
        "Simpler interpretation: ANOVA provides a single F-statistic and p-value, whereas multiple t-tests yield multiple p-values.\n",
        "\n",
        "Accounting for variance: ANOVA considers both between-group and within-group variance.\n",
        "\n",
        "Problems with Multiple t-tests:\n",
        "\n",
        "Inflated Type I error rate: Conducting multiple t-tests increases the likelihood of false positives.\n",
        "Multiple comparisons problem: Difficult to interpret and adjust for multiple p-values.\n",
        "Lack of consideration of variance: t-tests focus on pair-wise comparisons, neglecting overall variance.\n",
        "Additional benefits of One-way ANOVA:\n",
        "\n",
        "Post-hoc tests: Allows for pairwise comparisons after significant ANOVA results.\n",
        "Contrast analysis: Enables examination of specific group comparisons.\n",
        "Generalizability: ANOVA results are more generalizable to the population.\n",
        "Assumptions for One-way ANOVA:\n",
        "\n",
        "Normality of residuals.\n",
        "Homoscedasticity (equal variances).\n",
        "Independence of observations.\n",
        "Common applications:\n",
        "\n",
        "Comparing treatment outcomes.\n",
        "Analyzing survey responses.\n",
        "Evaluating differences in experimental conditions.\n"
      ],
      "metadata": {
        "id": "XU2_ZpJYxdmV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "mz0hCYA-xi56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> Variance Partitioning in ANOVA:\n",
        "\n",
        "ANOVA partitions the total variance in the data into two components:\n",
        "\n",
        "Between-Group Variance (SSB): Measures the variation between group means.\n",
        "Within-Group Variance (SSW): Measures the variation within each group.\n",
        "Total Variance (SST): Sum of SSB and SSW.\n",
        "\n",
        "Mathematical Representation:\n",
        "\n",
        "SST = SSB + SSW\n",
        "\n",
        "Breaking Down the Components:\n",
        "\n",
        "SSB (Between-Group Variance):\n",
        "Calculated as: Σn_i(μ_i - μ)^2\n",
        "Measures the sum of squared differences between group means and the overall mean.\n",
        "Degrees of freedom: k-1 (where k is the number of groups)\n",
        "SSW (Within-Group Variance):\n",
        "Calculated as: ΣΣ(x_ij - μ_i)^2\n",
        "Measures the sum of squared differences between individual observations and their group means.\n",
        "Degrees of freedom: N-k (where N is the total sample size)\n",
        "F-Statistic Calculation:\n",
        "\n",
        "The F-statistic is calculated as the ratio of between-group variance to within-group variance:\n",
        "\n",
        "F = (MSB / MSW)\n",
        "\n",
        "Where:\n",
        "\n",
        "MSB (Mean Square Between) = SSB / (k-1)\n",
        "MSW (Mean Square Within) = SSW / (N-k)\n",
        "Interpretation:\n",
        "\n",
        "A large F-statistic indicates significant differences between group means (SSB dominates).\n",
        "A small F-statistic indicates little difference between group means (SSW dominates).\n",
        "Key Concepts:\n",
        "\n",
        "Mean Square (MS): Average squared deviation.\n",
        "Degrees of Freedom: Number of independent pieces of information used to calculate variance.\n",
        "ANOVA Table:\n",
        "\n",
        "Source\tSS\tdf\tMS\tF\n",
        "Between Groups\tSSB\tk-1\tMSB\tF = MSB/MSW\n",
        "Within Groups\tSSW\tN-k\tMSW\n",
        "Total\tSST\tN-1"
      ],
      "metadata": {
        "id": "WX8-5lW6xv2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "pD05mWeDx3qE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANS--> Classical (Frequentist) Approach to ANOVA:\n",
        "\n",
        "Null Hypothesis Significance Testing (NHST): Tests whether the observed data support rejecting the null hypothesis.\n",
        "p-values: Measure the probability of observing the data (or more extreme) assuming the null hypothesis is true.\n",
        "α-level (e.g., 0.05): Arbitrary threshold for significance.\n",
        "Point estimates: Estimate population parameters (e.g., means) with a single value.\n",
        "Bayesian Approach to ANOVA:\n",
        "\n",
        "Probabilistic modeling: Updates probabilities based on data and prior knowledge.\n",
        "Bayes' theorem: Combines prior distributions with likelihood functions to estimate posterior distributions.\n",
        "Credible intervals: Quantify uncertainty around parameter estimates.\n",
        "Model comparison: Evaluates evidence for different models using Bayes factors.\n",
        "Key Differences:\n",
        "\n",
        "Uncertainty Handling:\n",
        "\n",
        "Frequentist: p-values and confidence intervals provide indirect measures of uncertainty.\n",
        "Bayesian: Posterior distributions and credible intervals directly quantify uncertainty.\n",
        "Parameter Estimation:\n",
        "\n",
        "Frequentist: Point estimates (e.g., sample mean).\n",
        "Bayesian: Distributional estimates (e.g., posterior distribution of population mean).\n",
        "Hypothesis Testing:\n",
        "\n",
        "Frequentist: Null hypothesis testing with p-values.\n",
        "Bayesian: Model comparison using Bayes factors or posterior probabilities.\n",
        "Additional Bayesian Advantages:\n",
        "\n",
        "Incorporating prior knowledge: Bayesian methods can incorporate expert knowledge or previous research.\n",
        "Flexibility: Bayesian models can handle complex data structures and non-normal distributions.\n",
        "Interpretability: Bayesian results provide direct probability statements about parameters.\n",
        "Challenges and Limitations:\n",
        "\n",
        "Computational complexity: Bayesian methods can be computationally intensive.\n",
        "Prior sensitivity: Results may depend on the choice of prior distributions.\n",
        "Interpretation: Requires understanding of Bayesian statistics and probability theory.\n",
        "Software for Bayesian ANOVA:\n",
        "\n",
        "R (e.g., brms, rstanarm)\n",
        "Python (e.g., PyMC3, scikit-bayes)\n",
        "JAGS\n",
        "Stan\n"
      ],
      "metadata": {
        "id": "V8L-Fwa1x8Gx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "\n",
        "Profession A: [48, 52, 55, 60, 62'\n",
        "Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "VufrMN3oyhwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Incomes for each profession\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate variances\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# Calculate F-statistic\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "# Calculate p-value (two-tailed test)\n",
        "dof_a = len(profession_a) - 1\n",
        "dof_b = len(profession_b) - 1\n",
        "p_value = 2 * min(stats.f.cdf(f_statistic, dof_a, dof_b), 1 - stats.f.cdf(f_statistic, dof_a, dof_b))\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpVSOQpUxQOi",
        "outputId": "dd0fa951-bcb3-4109-b88e-98d39046dd3a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.089171974522293\n",
            "p-value: 0.49304859900533904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data1\n",
        "\n",
        "Region A: [160, 162, 165, 158, 164]\n",
        "Region B: [172, 175, 170, 168, 174]\n",
        "Region C: [180, 182, 179, 185, 183]\n",
        "Task: Write Python code to perform the one-way ANOVA and interpret the result.\n",
        "Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n"
      ],
      "metadata": {
        "id": "EYhrCs_AzBs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Heights for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Output the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJ4LXhn0y7K9",
        "outputId": "35843302-0c49-48e9-86f0-57a79ab3c90e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ev7IkmcmzHMz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}